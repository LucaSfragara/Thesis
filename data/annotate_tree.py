from collections import defaultdict
from itertools import product
import sys
sys.path.append("..")
import numpy as np

import nltk
from data.grammars import GRAMMAR_CFG3b_string
from functools import lru_cache

from pprint import pprint

"""
22 -> 21 20
22 -> 20 19
19 -> 16 17 18
19 -> 17 18 16
20 -> 17 16 18
20 -> 16 17
21 -> 18 16
21 -> 16 18 17
16 -> 15 13
16 -> 13 15 14
17 -> 14 13 15
17 -> 15 13 14
18 -> 15 14 13
18 -> 14 13
13 -> 11 12
13 -> 12 11
14 -> 11 10 12
14 -> 10 11 12
15 -> 12 11 10
15 -> 11 12 10
10 -> 7 9 8
10 -> 9 8 7
11 -> 8 7 9
11 -> 7 8 9
12 -> 8 9 7
12 -> 9 7 8
7 -> "c" "a"
7 -> "a" "b" "c"
8 -> "c" "b"
8 -> "c" "a" "b"
9 -> "c" "b" "a"
9 -> "b" "a" 
"""


def parse_grammar(grammar_text):
    # First pass: collect all productions by LHS
    rules_temp = defaultdict(list)
    for line in grammar_text.strip().split('\n'):
        if not line.strip():
            continue  # skip empty lines
        lhs, rhs = line.split('->')
        lhs = lhs.strip()
        rhs_symbols = rhs.strip().split()
        rules_temp[lhs].append(rhs_symbols)
    
    # Second pass: assign uniform probabilities per LHS
    rules = {}
    for lhs, expansions in rules_temp.items():
        n = len(expansions)
        # Each rule for this LHS gets probability 1/n
        rules[lhs] = [(expansion, 1.0/n) for expansion in expansions]
    
    return rules

def generate_derivations(symbol, rules, rule_probs, max_depth=10):
    if symbol not in rules:  # it's a terminal
        yield [symbol], 1.0
    else:
        for i, expansion in enumerate(rules[symbol]):
            prob = rule_probs[symbol][i]
            
            # recursively expand each symbol in the RHS
            expansions = [generate_derivations(s, rules, rule_probs, max_depth-1) for s in expansion]
            for parts in product(*expansions):
                strings, probs = zip(*parts)
                yield sum(strings, []), prob * np.prod(probs)



grammar= parse_grammar(GRAMMAR_CFG3b_string)
pprint(grammar)



# Identify terminal symbols
nonterminals = set(grammar.keys())

all_symbols = {s for rhs_list in grammar.values() for rhs in rhs_list for s in rhs[0]}
terminals = all_symbols - nonterminals
print(all_symbols)
print("______")
print(terminals)
print("_______")
print(nonterminals)
print("_______")
# Recursive count with memoization
@lru_cache(maxsize=None)
def count_derivations(symbol):
    if symbol in terminals:
        return 1  # only one string: the terminal itself

    total = 0
    for rhs in grammar[symbol]:
        prod = 1
        for sym in rhs:
            prod *= count_derivations(sym)
        total += prod
    return total

# Total number of sentences from start symbol "22"
#print("Total derivable terminal strings from '22':", f"{count_derivations('22'):.{2}e}")

@lru_cache(maxsize=None)
def inside(symbol, s):
    """
    Computes the probability that `symbol` generates the string `s`.
    Here, `s` is a tuple of tokens.
    If symbol is a terminal, then inside(symbol, s) is 1 if s equals (symbol,), else 0.
    For a nonterminal, we sum over all rules.
    """
    # If symbol is terminal:

    if symbol in terminals:
        
        return 1.0 if s == (symbol,) else 0.0

    total = 0.0
    # Look up all productions for this nonterminal.
    for rhs, rule_prob in grammar[symbol]:
        # Let rhs be a list of symbols; we want to split the string `s` into parts
        # corresponding to the symbols in rhs.
        # We assume the grammar is binary/ternary and s is fully segmented.
        n = len(rhs)
        if n == 0:
            continue
        # If n==1, then we require that s is generated by that one symbol.
        if n == 1:
            total += rule_prob * inside(rhs[0], s)
        else:
            # For n>=2, try every possible way to split s into n parts.
            # For simplicity, we assume s has length L and use recursion over possible splits.
            L = len(s)
            # We need n-1 breakpoints. This can be done with recursion.
            total += rule_prob * split_inside(rhs, s)
    return total


def split_inside(rhs, s):
    """
    Given a list of symbols in rhs and a string s (tuple of tokens),
    return the sum over all ways of splitting s into len(rhs) parts,
    of the product of inside probabilities for each part.
    """
    if len(rhs) == 0:
        return 1.0 if len(s) == 0 else 0.0
    if len(rhs) == 1:
        return inside(rhs[0], s)
    
    total = 0.0
    L = len(s)
    # We try every split position for the first symbol.
    # The first part must be non-empty; the remaining must have at least len(rhs)-1 tokens.
    for split in range(1, L - len(rhs) + 2):
        first_part = s[:split]
        rest = s[split:]
        total += inside(rhs[0], first_part) * split_inside(rhs[1:], rest)
    return total


def next_token_prob(prefix, t, start_symbol="22"):
    # Represent prefix as a tuple of tokens.
    # Here we assume that our complete strings must end with an 'eos' token; if not, omit it.
    full = prefix + (t,)  # if you require an end-of-sequence marker, do: prefix + (t, "eos")
    return inside(start_symbol, full)

print(inside("9", ("c","b","a")))

"""
# Compute the unnormalized distribution:
prefix = ("a", )  # example prefix

cand_probs = {}
total = 0.0

for token in terminals:
    p = next_token_prob(prefix, token)
    cand_probs[token] = p
    print("Candidate Probabilities", cand_probs)
    print(total)
    total += p

# Normalize:
next_token_distribution = {token: p / total for token, p in cand_probs.items()}

print("Next token distribution:", next_token_distribution)
"""
inside.cache_clear()