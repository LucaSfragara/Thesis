===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
DecoderOnlyTransformer                        [256, 512, 5]             --
├─Embedding: 1-1                              [256, 512, 768]           3,840
├─PositionalEncoding: 1-2                     [256, 512, 768]           --
├─Dropout: 1-3                                [256, 512, 768]           --
├─ModuleList: 1-4                             --                        --
│    └─SelfAttentionDecoderLayer: 2-1         [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-1           --                        2,363,904
│    │    └─FeedForwardLayer: 3-2             [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-2         [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-3           --                        2,363,904
│    │    └─FeedForwardLayer: 3-4             [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-3         [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-5           --                        2,363,904
│    │    └─FeedForwardLayer: 3-6             [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-4         [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-7           --                        2,363,904
│    │    └─FeedForwardLayer: 3-8             [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-5         [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-9           --                        2,363,904
│    │    └─FeedForwardLayer: 3-10            [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-6         [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-11          --                        2,363,904
│    │    └─FeedForwardLayer: 3-12            [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-7         [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-13          --                        2,363,904
│    │    └─FeedForwardLayer: 3-14            [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-8         [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-15          --                        2,363,904
│    │    └─FeedForwardLayer: 3-16            [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-9         [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-17          --                        2,363,904
│    │    └─FeedForwardLayer: 3-18            [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-10        [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-19          --                        2,363,904
│    │    └─FeedForwardLayer: 3-20            [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-11        [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-21          --                        2,363,904
│    │    └─FeedForwardLayer: 3-22            [256, 512, 768]           4,723,968
│    └─SelfAttentionDecoderLayer: 2-12        [256, 512, 768]           --
│    │    └─SelfAttentionLayer: 3-23          --                        2,363,904
│    │    └─FeedForwardLayer: 3-24            [256, 512, 768]           4,723,968
├─LayerNorm: 1-5                              [256, 512, 768]           1,536
├─Linear: 1-6                                 [256, 512, 5]             3,845
===============================================================================================
Total params: 85,063,685
Trainable params: 85,063,685
Non-trainable params: 0
Total mult-adds (G): 14.52
===============================================================================================
Input size (MB): 1.05
Forward/backward pass size (MB): 69261.59
Params size (MB): 226.86
Estimated Total Size (MB): 69489.50
===============================================================================================